{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw07.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_PcWu8RsKnG",
        "outputId": "b8fcd945-d482-400b-c86a-7605548edb17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "customSchema = StructType([\n",
        "      StructField(\"Date\",StringType(),True),\n",
        "      StructField(\"Location\",StringType(),True),\n",
        "      StructField(\"MinTemp\",FloatType(),True),\n",
        "      StructField(\"MaxTemp\",FloatType(),True),\n",
        "      StructField(\"Rainfall\",FloatType(),True),\n",
        "      StructField(\"Evaporation\",StringType(),True),\n",
        "      StructField(\"Sunshine\",StringType(),True),\n",
        "      StructField(\"WindGustDir\",StringType(),True),\n",
        "      StructField(\"WindGustSpeed\",FloatType(),True),\n",
        "      StructField(\"WindDir9am\",StringType(),True),\n",
        "      StructField(\"WindDir3pm\",StringType(),True),\n",
        "      StructField(\"WindSpeed9am\",FloatType(),True),\n",
        "      StructField(\"WindSpeed3pm\",FloatType(),True),\n",
        "      StructField(\"Humidity9am\",FloatType(),True),\n",
        "      StructField(\"Humidity3pm\",FloatType(),True),\n",
        "      StructField(\"Pressure9am\",FloatType(),True),\n",
        "      StructField(\"Pressure3pm\",FloatType(),True),\n",
        "      StructField(\"Cloud9am\",FloatType(),True),\n",
        "      StructField(\"Cloud3pm\",FloatType(),True),\n",
        "      StructField(\"Temp9am\",FloatType(),True),\n",
        "      StructField(\"Temp3pm\",FloatType(),True),\n",
        "      StructField(\"RainToday\",StringType(),True),\n",
        "      StructField(\"RainTomorrow\",StringType(),True)\n",
        "])\n",
        "# Defining Custom Schema"
      ],
      "metadata": {
        "id": "-6vF4deqsUOO"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### \"Rain in Australia\" Prediction"
      ],
      "metadata": {
        "id": "lkMxQCvlHTY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "FSua88twHmdc"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the data and setting the NA as Null values as NA\n",
        "data = spark.read.csv(\"weatherAUS.csv\", header=\"true\", schema=customSchema,nullValue= 'NA')\n",
        "data = data.drop(\"Date\", \"Evaporation\",\"Sunshine\",\"Cloud9am\", \"Cloud3pm\", 'WindGustDir', 'WindGustSpeed')\n"
      ],
      "metadata": {
        "id": "H9l_1pfAsWSu"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.na.drop() # Dropping the Null values\n",
        "data.show()"
      ],
      "metadata": {
        "id": "W34yiDX8shmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data 80/20 train/test, using a seed of 12345\n",
        "(train, test) = data.randomSplit([0.8, 0.2])"
      ],
      "metadata": {
        "id": "q0laPxpTsjki"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing the categoricalColumns i.e., String columns except the RainTomorrow\n",
        "categoricalColumns = [\"Location\", \"WindDir9am\", \"WindDir3pm\", \"RainToday\"]"
      ],
      "metadata": {
        "id": "PZZvP7jaslY7"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stages = [] # stages in Pipeline"
      ],
      "metadata": {
        "id": "rhnFu5evsoM8"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert categorical variables into one-hot encoded variables"
      ],
      "metadata": {
        "id": "XNLNoePxQ9m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "for categoricalCol in categoricalColumns:\n",
        "    # Category Indexing with StringIndexer\n",
        "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
        "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
        "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
        "    # Add stages.\n",
        "    stages += [stringIndexer, encoder]"
      ],
      "metadata": {
        "id": "gChcCuQ_snMi"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stages"
      ],
      "metadata": {
        "id": "mDK_C9sZtYVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1606392-eef4-49cb-b8f0-5c5d2b6f6908"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[StringIndexer_f46cc6a5689f,\n",
              " OneHotEncoder_54aa1465a250,\n",
              " StringIndexer_2e36fd8bf3da,\n",
              " OneHotEncoder_10bdb3b1cfc5,\n",
              " StringIndexer_62226c854877,\n",
              " OneHotEncoder_249850633b0b,\n",
              " StringIndexer_3be38c7d41d2,\n",
              " OneHotEncoder_45dc978f049e]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the label column\n",
        "label_stringIdx = StringIndexer(inputCol=\"RainTomorrow\", outputCol=\"label\")\n",
        "# setHandleInvalid(\"skip\"), the indexer adds new indexes when it sees new labels.\n",
        "stages += [label_stringIdx]"
      ],
      "metadata": {
        "id": "v4JwakKftewl"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform all features into a vector using VectorAssembler\n",
        "numericCols = [\"MinTemp\", \"MaxTemp\", \"WindSpeed9am\", \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\", \"Temp9am\", \"Temp3pm\", \"Pressure9am\", \"Pressure3pm\"]\n",
        "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
        "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
        "stages += [assembler]\n",
        "dtree = DecisionTreeClassifier(labelCol=\"label\", featuresCol=assembler.getOutputCol())\n"
      ],
      "metadata": {
        "id": "0cYQPlRURNeU"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a parameter grid to determine the best parameters for:\n",
        "impurity - gini, entropy\n",
        "maxBins - 5, 10, 15\n",
        "minInfoGain - 0.0, 0.2, 0.4\n",
        "maxDepth - 3, 5, 7"
      ],
      "metadata": {
        "id": "OK1-86WvRPNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paramGrid = (ParamGridBuilder()\n",
        "    .addGrid(dtree.impurity, ['gini', 'entropy'])\n",
        "    .addGrid(dtree.maxBins, [5, 10, 15])\n",
        "    .addGrid(dtree.minInfoGain, [0.0, 0.2, 0.4])\n",
        "    .addGrid(dtree.maxDepth, [3, 5, 7])\n",
        "    .build())"
      ],
      "metadata": {
        "id": "AvFcZKa5toAX"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = BinaryClassificationEvaluator()"
      ],
      "metadata": {
        "id": "rv6bLntlRXY3"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validate with 4 folds"
      ],
      "metadata": {
        "id": "vIkDUVMiRdm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CrossValidator(estimator=dtree, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=4)\n",
        "stages += [cv]"
      ],
      "metadata": {
        "id": "kr7ec0kdRW7s"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline"
      ],
      "metadata": {
        "id": "DMaUc6FhNAoM"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Use a pipeline to encapsulate all steps"
      ],
      "metadata": {
        "id": "hRqp9rfpRhbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the pipeline with all the above steps\n",
        "pipeline = Pipeline().setStages(stages)\n"
      ],
      "metadata": {
        "id": "chY6sU1uud-Y"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_model = pipeline.fit(train)\n",
        "predictions = pipeline_model.transform(test)"
      ],
      "metadata": {
        "id": "x81vCvaRQ2bt"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Print the parameters from the best model selected"
      ],
      "metadata": {
        "id": "h1qC1ZbCRzoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = pipeline_model.stages[-1].bestModel"
      ],
      "metadata": {
        "id": "kyiDNeBTxhqj"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_modelobj = best_model._java_obj.parent()\n",
        "\n",
        "best_modeldepth = best_modelobj.getMaxDepth()\n",
        "best_modelbins = best_modelobj.getMaxBins()\n",
        "best_modelimpurity = best_modelobj.getImpurity()\n",
        "best_modelgain = best_modelobj.getMinInfoGain()"
      ],
      "metadata": {
        "id": "pPtiLwG3yFQ1"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best model grid params are \")\n",
        "print(best_modeldepth)\n",
        "print(best_modelbins)\n",
        "print(best_modelimpurity)\n",
        "print(best_modelgain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjK5pk76yMWU",
        "outputId": "ca4e1143-2184-44ae-ad96-3ba5466b1913"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model grid params are \n",
            "7\n",
            "10\n",
            "entropy\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate and print the Area under ROC Curve and Area under Precision-Recall Curve scores for your training and test data sets"
      ],
      "metadata": {
        "id": "cXAtYtx4Sixt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator.evaluate(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT7cy5yF26QP",
        "outputId": "58fc9916-1d24-4e74-9b87-6d4fb637d985"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.42225775465215"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Area under ROC Curve"
      ],
      "metadata": {
        "id": "HNPIITnNSmXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = BinaryClassificationEvaluator()\n",
        "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DhTd_OO1yxw",
        "outputId": "210ddb5e-6b4a-4133-b4a3-b1f3eb93f48d"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Area Under ROC: 0.42225775465215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Area under Precision-Recall"
      ],
      "metadata": {
        "id": "ANFdVCz9SorV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator2 = BinaryClassificationEvaluator()\n",
        "print(\"Test Area Under PR: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxLTtU3J2v6W",
        "outputId": "3dc496cd-f962-4b4d-e617-0d95f43036e9"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Area Under PR: 0.27348693322723666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://docs.databricks.com/_static/notebooks/binary-classification.html\n",
        "\n",
        "https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa"
      ],
      "metadata": {
        "id": "Jt7jtChC3ISp"
      }
    }
  ]
}